{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KMwYLYlwstf",
        "outputId": "d3e87557-c960-428f-9e02-d8e821aa6a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Hybrid-ABSA'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 40 (delta 18), reused 30 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (40/40), 17.29 KiB | 4.32 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/ali-sedaghi/Hybrid-ABSA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx95UVWZCte9",
        "outputId": "5f8b65f5-dc18-4e6e-e45c-3210ab4d0a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Hybrid-ABSA\n"
          ]
        }
      ],
      "source": [
        "! cd Hybrid-ABSA/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGEgPd4Fz73",
        "outputId": "b886133c-3d46-4fc3-9e56-14c92c8f0801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "From https://github.com/ali-sedaghi/Hybrid-ABSA\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "! git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM91vB0gCyuA",
        "outputId": "777a86f2-4228-4df9-bb45-5d39eb008c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-02-06 18:18:43,063 - INFO - --- STARTING EXPERIMENT ---\n",
            "2026-02-06 18:18:43,064 - INFO - Loading dataset from dataset.csv...\n",
            "2026-02-06 18:18:43,074 - INFO - Loaded 11 examples.\n",
            "2026-02-06 18:18:43,074 - INFO - Initializing Instruct-DeBERTa (Beam Size = 1)...\n",
            "[Instruct-DeBERTa] Loading on cpu (Beam Size: 1)...\n",
            "2026-02-06 18:18:43,221 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:43,261 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:43,307 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "config.json: 100% 776/776 [00:00<00:00, 2.03MB/s]\n",
            "2026-02-06 18:18:43,376 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:43,418 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:43,461 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "tokenizer_config.json: 2.57kB [00:00, 3.57MB/s]\n",
            "2026-02-06 18:18:43,516 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:43,592 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:43,641 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/spiece.model \"HTTP/1.1 302 Found\"\n",
            "2026-02-06 18:18:43,742 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/xet-read-token/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2 \"HTTP/1.1 200 OK\"\n",
            "spiece.model: 100% 792k/792k [00:01<00:00, 545kB/s]\n",
            "2026-02-06 18:18:45,244 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/tokenizer.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:45,288 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/tokenizer.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:45,332 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/tokenizer.json \"HTTP/1.1 200 OK\"\n",
            "tokenizer.json: 2.42MB [00:00, 47.5MB/s]\n",
            "2026-02-06 18:18:45,438 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:45,487 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:45,529 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:45,572 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
            "special_tokens_map.json: 2.20kB [00:00, 54.4kB/s]\n",
            "2026-02-06 18:18:45,673 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "2026-02-06 18:18:45,674 - WARNING - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "2026-02-06 18:18:46,387 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:46,405 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:46,457 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:46,550 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:46,568 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:46,659 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:46,874 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:47,018 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/pytorch_model.bin \"HTTP/1.1 302 Found\"\n",
            "pytorch_model.bin: 100% 990M/990M [00:08<00:00, 117MB/s]\n",
            "2026-02-06 18:18:55,635 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:55,687 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:55,777 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/commits/main \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:55,837 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "Loading weights:  45% 129/284 [00:00<00:00, 2176.60it/s, Materializing param=decoder.block.9.layer.0.SelfAttention.o.weight]2026-02-06 18:18:55,916 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "Loading weights:  68% 194/284 [00:00<00:00, 1090.29it/s, Materializing param=encoder.block.2.layer.0.SelfAttention.v.weight]2026-02-06 18:18:56,015 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "Loading weights:  81% 231/284 [00:00<00:00, 1090.29it/s, Materializing param=encoder.block.6.layer.0.layer_norm.weight]2026-02-06 18:18:56,066 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/refs%2Fpr%2F1/model.safetensors \"HTTP/1.1 302 Found\"\n",
            "Loading weights: 100% 284/284 [00:00<00:00, 1063.43it/s, Materializing param=shared.weight]\n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "2026-02-06 18:18:56,153 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/xet-read-token/a27f14d1ee41c2058cdb696d97c68da201bd111f \"HTTP/1.1 200 OK\"\n",
            "model.safetensors:   0% 0.00/990M [00:00<?, ?B/s]2026-02-06 18:18:56,219 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:56,260 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:56,301 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "\n",
            "generation_config.json: 100% 142/142 [00:00<00:00, 593kB/s]\n",
            "2026-02-06 18:18:56,361 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:56,378 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:56,397 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/config.json \"HTTP/1.1 200 OK\"\n",
            "\n",
            "config.json: 1.03kB [00:00, 2.38MB/s]\n",
            "2026-02-06 18:18:56,449 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:18:56,601 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:18:56,618 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:18:56,671 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
            "2026-02-06 18:18:56,718 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1/xet-read-token/10c9dff335a44073e1352360c3a7bc54dc58eb01 \"HTTP/1.1 200 OK\"\n",
            "\n",
            "model.safetensors:   0% 0.00/738M [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   9% 67.1M/738M [00:02<00:27, 24.8MB/s]\u001b[A\n",
            "model.safetensors:  39% 387M/990M [00:08<00:08, 73.5MB/s]\n",
            "model.safetensors:  46% 454M/990M [00:09<00:06, 78.6MB/s]\n",
            "model.safetensors:  36% 268M/738M [00:09<00:15, 30.8MB/s]\u001b[A\n",
            "model.safetensors:  45% 335M/738M [00:09<00:08, 47.0MB/s]\u001b[A\n",
            "model.safetensors:  55% 402M/738M [00:09<00:05, 66.9MB/s]\u001b[A\n",
            "model.safetensors:  64% 470M/738M [00:09<00:02, 91.3MB/s]\u001b[A\n",
            "model.safetensors:  73% 537M/738M [00:09<00:01, 125MB/s] \u001b[A\n",
            "model.safetensors:  82% 604M/738M [00:10<00:00, 141MB/s]\u001b[A\n",
            "model.safetensors:  91% 671M/738M [00:10<00:00, 173MB/s]\u001b[A\n",
            "model.safetensors: 100% 738M/738M [00:10<00:00, 71.2MB/s]\n",
            "\n",
            "Loading weights:   0% 0/202 [00:00<?, ?it/s]\u001b[A\n",
            "Loading weights:   0% 1/202 [00:00<00:00, 13273.11it/s, Materializing param=classifier.bias]\u001b[A\n",
            "Loading weights:   0% 1/202 [00:00<00:00, 5090.17it/s, Materializing param=classifier.bias] \u001b[A\n",
            "Loading weights:   1% 2/202 [00:00<00:00, 4027.18it/s, Materializing param=classifier.weight]\u001b[A\n",
            "Loading weights:   1% 2/202 [00:00<00:00, 3315.66it/s, Materializing param=classifier.weight]\u001b[A\n",
            "Loading weights:   1% 3/202 [00:00<00:00, 3178.31it/s, Materializing param=deberta.embeddings.LayerNorm.bias]\u001b[A\n",
            "Loading weights:   1% 3/202 [00:00<00:00, 2000.14it/s, Materializing param=deberta.embeddings.LayerNorm.bias]\u001b[A\n",
            "Loading weights:   2% 4/202 [00:00<00:00, 1001.33it/s, Materializing param=deberta.embeddings.LayerNorm.weight]\u001b[A\n",
            "Loading weights:   2% 4/202 [00:00<00:00, 945.89it/s, Materializing param=deberta.embeddings.LayerNorm.weight] \u001b[A\n",
            "Loading weights:   2% 5/202 [00:00<00:00, 1085.31it/s, Materializing param=deberta.embeddings.word_embeddings.weight]\u001b[A\n",
            "Loading weights:   2% 5/202 [00:00<00:00, 1054.27it/s, Materializing param=deberta.embeddings.word_embeddings.weight]\u001b[A\n",
            "Loading weights:   3% 6/202 [00:00<00:00, 984.96it/s, Materializing param=deberta.encoder.LayerNorm.bias]            \u001b[A\n",
            "Loading weights:   3% 6/202 [00:00<00:00, 961.15it/s, Materializing param=deberta.encoder.LayerNorm.bias]\u001b[A\n",
            "Loading weights:   3% 7/202 [00:00<00:00, 1067.41it/s, Materializing param=deberta.encoder.LayerNorm.weight]\u001b[A\n",
            "Loading weights:   3% 7/202 [00:00<00:00, 1049.66it/s, Materializing param=deberta.encoder.LayerNorm.weight]\u001b[A\n",
            "Loading weights:   4% 8/202 [00:00<00:00, 1054.24it/s, Materializing param=deberta.encoder.layer.0.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:   4% 8/202 [00:00<00:00, 1034.80it/s, Materializing param=deberta.encoder.layer.0.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:   4% 9/202 [00:00<00:00, 915.88it/s, Materializing param=deberta.encoder.layer.0.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:   4% 9/202 [00:00<00:00, 813.73it/s, Materializing param=deberta.encoder.layer.0.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:   5% 10/202 [00:00<00:00, 875.78it/s, Materializing param=deberta.encoder.layer.0.attention.output.dense.bias]     \u001b[A\n",
            "Loading weights:   5% 10/202 [00:00<00:00, 865.72it/s, Materializing param=deberta.encoder.layer.0.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:   5% 11/202 [00:00<00:00, 930.24it/s, Materializing param=deberta.encoder.layer.0.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:   5% 11/202 [00:00<00:00, 892.37it/s, Materializing param=deberta.encoder.layer.0.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:   6% 12/202 [00:00<00:00, 863.22it/s, Materializing param=deberta.encoder.layer.0.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:   6% 12/202 [00:00<00:00, 855.20it/s, Materializing param=deberta.encoder.layer.0.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:   6% 13/202 [00:00<00:00, 809.43it/s, Materializing param=deberta.encoder.layer.0.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:   6% 13/202 [00:00<00:00, 800.35it/s, Materializing param=deberta.encoder.layer.0.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:   7% 14/202 [00:00<00:00, 677.73it/s, Materializing param=deberta.encoder.layer.0.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:   7% 14/202 [00:00<00:00, 670.86it/s, Materializing param=deberta.encoder.layer.0.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:   7% 15/202 [00:00<00:00, 701.15it/s, Materializing param=deberta.encoder.layer.0.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:   7% 15/202 [00:00<00:00, 667.65it/s, Materializing param=deberta.encoder.layer.0.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:   8% 16/202 [00:00<00:00, 648.77it/s, Materializing param=deberta.encoder.layer.0.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:   8% 16/202 [00:00<00:00, 643.69it/s, Materializing param=deberta.encoder.layer.0.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:   8% 17/202 [00:00<00:00, 674.41it/s, Materializing param=deberta.encoder.layer.0.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:   8% 17/202 [00:00<00:00, 625.53it/s, Materializing param=deberta.encoder.layer.0.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:   9% 18/202 [00:00<00:00, 653.10it/s, Materializing param=deberta.encoder.layer.0.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:   9% 18/202 [00:00<00:00, 650.43it/s, Materializing param=deberta.encoder.layer.0.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:   9% 19/202 [00:00<00:00, 678.33it/s, Materializing param=deberta.encoder.layer.0.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:   9% 19/202 [00:00<00:00, 674.11it/s, Materializing param=deberta.encoder.layer.0.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  10% 20/202 [00:00<00:00, 633.79it/s, Materializing param=deberta.encoder.layer.0.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  10% 20/202 [00:00<00:00, 630.70it/s, Materializing param=deberta.encoder.layer.0.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  10% 21/202 [00:00<00:00, 620.45it/s, Materializing param=deberta.encoder.layer.0.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  10% 21/202 [00:00<00:00, 597.30it/s, Materializing param=deberta.encoder.layer.0.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  11% 22/202 [00:00<00:00, 618.62it/s, Materializing param=deberta.encoder.layer.0.output.dense.bias]      \u001b[A\n",
            "Loading weights:  11% 22/202 [00:00<00:00, 600.34it/s, Materializing param=deberta.encoder.layer.0.output.dense.bias]\u001b[A\n",
            "Loading weights:  11% 23/202 [00:00<00:00, 621.62it/s, Materializing param=deberta.encoder.layer.0.output.dense.weight]\u001b[A\n",
            "Loading weights:  11% 23/202 [00:00<00:00, 619.27it/s, Materializing param=deberta.encoder.layer.0.output.dense.weight]\u001b[A\n",
            "Loading weights:  12% 24/202 [00:00<00:00, 625.91it/s, Materializing param=deberta.encoder.layer.1.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  12% 24/202 [00:00<00:00, 623.77it/s, Materializing param=deberta.encoder.layer.1.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  12% 25/202 [00:00<00:00, 644.30it/s, Materializing param=deberta.encoder.layer.1.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  12% 25/202 [00:00<00:00, 642.23it/s, Materializing param=deberta.encoder.layer.1.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  13% 26/202 [00:00<00:00, 663.31it/s, Materializing param=deberta.encoder.layer.1.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  13% 26/202 [00:00<00:00, 589.74it/s, Materializing param=deberta.encoder.layer.1.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  13% 27/202 [00:00<00:00, 605.87it/s, Materializing param=deberta.encoder.layer.1.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  13% 27/202 [00:00<00:00, 603.92it/s, Materializing param=deberta.encoder.layer.1.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  14% 28/202 [00:00<00:00, 597.41it/s, Materializing param=deberta.encoder.layer.1.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  14% 28/202 [00:00<00:00, 594.97it/s, Materializing param=deberta.encoder.layer.1.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  14% 29/202 [00:00<00:00, 595.33it/s, Materializing param=deberta.encoder.layer.1.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  14% 29/202 [00:00<00:00, 592.30it/s, Materializing param=deberta.encoder.layer.1.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  15% 30/202 [00:00<00:00, 598.28it/s, Materializing param=deberta.encoder.layer.1.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  15% 30/202 [00:00<00:00, 596.34it/s, Materializing param=deberta.encoder.layer.1.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  15% 31/202 [00:00<00:00, 577.18it/s, Materializing param=deberta.encoder.layer.1.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  15% 31/202 [00:00<00:00, 575.44it/s, Materializing param=deberta.encoder.layer.1.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  16% 32/202 [00:00<00:00, 589.92it/s, Materializing param=deberta.encoder.layer.1.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  16% 32/202 [00:00<00:00, 588.14it/s, Materializing param=deberta.encoder.layer.1.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  16% 33/202 [00:00<00:00, 602.62it/s, Materializing param=deberta.encoder.layer.1.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  16% 33/202 [00:00<00:00, 601.38it/s, Materializing param=deberta.encoder.layer.1.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  17% 34/202 [00:00<00:00, 616.22it/s, Materializing param=deberta.encoder.layer.1.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  17% 34/202 [00:00<00:00, 565.52it/s, Materializing param=deberta.encoder.layer.1.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  17% 35/202 [00:00<00:00, 549.00it/s, Materializing param=deberta.encoder.layer.1.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  17% 35/202 [00:00<00:00, 547.30it/s, Materializing param=deberta.encoder.layer.1.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  18% 36/202 [00:00<00:00, 559.00it/s, Materializing param=deberta.encoder.layer.1.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  18% 36/202 [00:00<00:00, 543.19it/s, Materializing param=deberta.encoder.layer.1.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  18% 37/202 [00:00<00:00, 554.80it/s, Materializing param=deberta.encoder.layer.1.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  18% 37/202 [00:00<00:00, 553.91it/s, Materializing param=deberta.encoder.layer.1.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  19% 38/202 [00:00<00:00, 566.72it/s, Materializing param=deberta.encoder.layer.1.output.dense.bias]      \u001b[A\n",
            "Loading weights:  19% 38/202 [00:00<00:00, 565.80it/s, Materializing param=deberta.encoder.layer.1.output.dense.bias]\u001b[A\n",
            "Loading weights:  19% 39/202 [00:00<00:00, 560.10it/s, Materializing param=deberta.encoder.layer.1.output.dense.weight]\u001b[A\n",
            "Loading weights:  19% 39/202 [00:00<00:00, 558.88it/s, Materializing param=deberta.encoder.layer.1.output.dense.weight]\u001b[A\n",
            "Loading weights:  20% 40/202 [00:00<00:00, 551.87it/s, Materializing param=deberta.encoder.layer.2.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  20% 40/202 [00:00<00:00, 550.69it/s, Materializing param=deberta.encoder.layer.2.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  20% 41/202 [00:00<00:00, 562.08it/s, Materializing param=deberta.encoder.layer.2.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  20% 41/202 [00:00<00:00, 561.24it/s, Materializing param=deberta.encoder.layer.2.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  21% 42/202 [00:00<00:00, 564.40it/s, Materializing param=deberta.encoder.layer.2.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  21% 42/202 [00:00<00:00, 562.98it/s, Materializing param=deberta.encoder.layer.2.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  21% 43/202 [00:00<00:00, 554.58it/s, Materializing param=deberta.encoder.layer.2.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  21% 43/202 [00:00<00:00, 552.92it/s, Materializing param=deberta.encoder.layer.2.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  22% 44/202 [00:00<00:00, 562.66it/s, Materializing param=deberta.encoder.layer.2.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  22% 44/202 [00:00<00:00, 561.65it/s, Materializing param=deberta.encoder.layer.2.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  22% 45/202 [00:00<00:00, 572.33it/s, Materializing param=deberta.encoder.layer.2.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  22% 45/202 [00:00<00:00, 571.44it/s, Materializing param=deberta.encoder.layer.2.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  23% 46/202 [00:00<00:00, 581.95it/s, Materializing param=deberta.encoder.layer.2.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  23% 46/202 [00:00<00:00, 580.94it/s, Materializing param=deberta.encoder.layer.2.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  23% 47/202 [00:00<00:00, 591.69it/s, Materializing param=deberta.encoder.layer.2.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  23% 47/202 [00:00<00:00, 590.84it/s, Materializing param=deberta.encoder.layer.2.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  24% 48/202 [00:00<00:00, 601.23it/s, Materializing param=deberta.encoder.layer.2.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  24% 48/202 [00:00<00:00, 600.25it/s, Materializing param=deberta.encoder.layer.2.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  24% 49/202 [00:00<00:00, 587.46it/s, Materializing param=deberta.encoder.layer.2.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  24% 49/202 [00:00<00:00, 586.42it/s, Materializing param=deberta.encoder.layer.2.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  25% 50/202 [00:00<00:00, 596.33it/s, Materializing param=deberta.encoder.layer.2.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  25% 50/202 [00:00<00:00, 595.42it/s, Materializing param=deberta.encoder.layer.2.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  25% 51/202 [00:00<00:00, 604.96it/s, Materializing param=deberta.encoder.layer.2.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  25% 51/202 [00:00<00:00, 604.11it/s, Materializing param=deberta.encoder.layer.2.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  26% 52/202 [00:00<00:00, 613.84it/s, Materializing param=deberta.encoder.layer.2.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  26% 52/202 [00:00<00:00, 613.00it/s, Materializing param=deberta.encoder.layer.2.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  26% 53/202 [00:00<00:00, 622.50it/s, Materializing param=deberta.encoder.layer.2.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  26% 53/202 [00:00<00:00, 621.52it/s, Materializing param=deberta.encoder.layer.2.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  27% 54/202 [00:00<00:00, 618.51it/s, Materializing param=deberta.encoder.layer.2.output.dense.bias]      \u001b[A\n",
            "Loading weights:  27% 54/202 [00:00<00:00, 617.13it/s, Materializing param=deberta.encoder.layer.2.output.dense.bias]\u001b[A\n",
            "model.safetensors:  53% 521M/990M [00:11<00:09, 48.7MB/s]\n",
            "Loading weights:  27% 55/202 [00:00<00:00, 623.21it/s, Materializing param=deberta.encoder.layer.2.output.dense.weight]\u001b[A\n",
            "Loading weights:  28% 56/202 [00:00<00:00, 632.11it/s, Materializing param=deberta.encoder.layer.3.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  28% 56/202 [00:00<00:00, 631.20it/s, Materializing param=deberta.encoder.layer.3.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  28% 57/202 [00:00<00:00, 639.85it/s, Materializing param=deberta.encoder.layer.3.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  28% 57/202 [00:00<00:00, 638.82it/s, Materializing param=deberta.encoder.layer.3.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  29% 58/202 [00:00<00:00, 641.56it/s, Materializing param=deberta.encoder.layer.3.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  29% 58/202 [00:00<00:00, 640.72it/s, Materializing param=deberta.encoder.layer.3.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  29% 59/202 [00:00<00:00, 638.90it/s, Materializing param=deberta.encoder.layer.3.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  29% 59/202 [00:00<00:00, 637.79it/s, Materializing param=deberta.encoder.layer.3.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  30% 60/202 [00:00<00:00, 646.36it/s, Materializing param=deberta.encoder.layer.3.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  30% 60/202 [00:00<00:00, 645.46it/s, Materializing param=deberta.encoder.layer.3.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  30% 61/202 [00:00<00:00, 653.98it/s, Materializing param=deberta.encoder.layer.3.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  30% 61/202 [00:00<00:00, 653.07it/s, Materializing param=deberta.encoder.layer.3.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  31% 62/202 [00:00<00:00, 661.65it/s, Materializing param=deberta.encoder.layer.3.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  31% 62/202 [00:00<00:00, 660.89it/s, Materializing param=deberta.encoder.layer.3.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  31% 63/202 [00:00<00:00, 652.04it/s, Materializing param=deberta.encoder.layer.3.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  31% 63/202 [00:00<00:00, 650.75it/s, Materializing param=deberta.encoder.layer.3.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  32% 64/202 [00:00<00:00, 659.01it/s, Materializing param=deberta.encoder.layer.3.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  32% 64/202 [00:00<00:00, 657.83it/s, Materializing param=deberta.encoder.layer.3.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  32% 65/202 [00:00<00:00, 666.52it/s, Materializing param=deberta.encoder.layer.3.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  32% 65/202 [00:00<00:00, 665.85it/s, Materializing param=deberta.encoder.layer.3.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  33% 66/202 [00:00<00:00, 674.79it/s, Materializing param=deberta.encoder.layer.3.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  33% 66/202 [00:00<00:00, 674.02it/s, Materializing param=deberta.encoder.layer.3.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  33% 67/202 [00:00<00:00, 682.81it/s, Materializing param=deberta.encoder.layer.3.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  33% 67/202 [00:00<00:00, 682.15it/s, Materializing param=deberta.encoder.layer.3.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  34% 68/202 [00:00<00:00, 690.85it/s, Materializing param=deberta.encoder.layer.3.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  34% 68/202 [00:00<00:00, 690.17it/s, Materializing param=deberta.encoder.layer.3.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  34% 69/202 [00:00<00:00, 698.96it/s, Materializing param=deberta.encoder.layer.3.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  34% 69/202 [00:00<00:00, 687.49it/s, Materializing param=deberta.encoder.layer.3.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  35% 70/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.3.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  35% 70/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.3.output.dense.bias]      \u001b[A\n",
            "Loading weights:  35% 70/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.3.output.dense.bias]\u001b[A\n",
            "Loading weights:  35% 71/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.3.output.dense.weight]\u001b[A\n",
            "Loading weights:  35% 71/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.3.output.dense.weight]\u001b[A\n",
            "Loading weights:  36% 72/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  36% 72/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  36% 73/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  36% 73/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  37% 74/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  37% 74/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  37% 75/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  37% 75/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  38% 76/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  38% 76/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  38% 77/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  38% 77/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  39% 78/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  39% 78/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  39% 79/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  39% 79/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  40% 80/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  40% 80/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  40% 81/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  40% 81/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  41% 82/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  41% 82/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  41% 83/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  41% 83/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  42% 84/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  42% 84/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  42% 85/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  42% 85/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  43% 86/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.dense.bias]      \u001b[A\n",
            "Loading weights:  43% 86/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.dense.bias]\u001b[A\n",
            "Loading weights:  43% 87/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.dense.weight]\u001b[A\n",
            "Loading weights:  43% 87/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.4.output.dense.weight]\u001b[A\n",
            "Loading weights:  44% 88/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  44% 88/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  44% 89/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  44% 89/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  45% 90/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  45% 90/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  45% 91/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  45% 91/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  46% 92/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  46% 92/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  46% 93/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  46% 93/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  47% 94/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  47% 94/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  47% 95/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  47% 95/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  48% 96/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  48% 96/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  48% 97/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  48% 97/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  49% 98/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  49% 98/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  49% 99/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  49% 99/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  50% 100/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.LayerNorm.bias]   \u001b[A\n",
            "Loading weights:  50% 100/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  50% 101/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  50% 101/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  50% 102/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.dense.bias]      \u001b[A\n",
            "Loading weights:  50% 102/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.dense.bias]\u001b[A\n",
            "Loading weights:  51% 103/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.dense.weight]\u001b[A\n",
            "Loading weights:  51% 103/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.5.output.dense.weight]\u001b[A\n",
            "Loading weights:  51% 104/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  51% 104/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  52% 105/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  52% 105/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  52% 106/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  52% 106/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  53% 107/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  53% 107/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  53% 108/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  53% 108/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  54% 109/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  54% 109/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  54% 110/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  54% 110/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  55% 111/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  55% 111/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  55% 112/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  55% 112/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  56% 113/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  56% 113/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  56% 114/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  56% 114/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  57% 115/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  57% 115/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  57% 116/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  57% 116/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  58% 117/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  58% 117/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  58% 118/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.dense.bias]      \u001b[A\n",
            "Loading weights:  58% 118/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.dense.bias]\u001b[A\n",
            "Loading weights:  59% 119/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.dense.weight]\u001b[A\n",
            "Loading weights:  59% 119/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.6.output.dense.weight]\u001b[A\n",
            "Loading weights:  59% 120/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  59% 120/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  60% 121/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  60% 121/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  60% 122/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  60% 122/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  61% 123/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  61% 123/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  61% 124/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  61% 124/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  62% 125/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  62% 125/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  62% 126/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  62% 126/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  63% 127/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  63% 127/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  63% 128/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  63% 128/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  64% 129/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  64% 129/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  64% 130/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  64% 130/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  65% 131/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  65% 131/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  65% 132/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  65% 132/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  66% 133/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  66% 133/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  66% 134/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.dense.bias]      \u001b[A\n",
            "Loading weights:  66% 134/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.dense.bias]\u001b[A\n",
            "Loading weights:  67% 135/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.dense.weight]\u001b[A\n",
            "Loading weights:  67% 135/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.7.output.dense.weight]\u001b[A\n",
            "Loading weights:  67% 136/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  67% 136/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  68% 137/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  68% 137/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  68% 138/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  68% 138/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  69% 139/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  69% 139/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  69% 140/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  69% 140/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  70% 141/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  70% 141/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  70% 142/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  70% 142/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  71% 143/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  71% 143/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  71% 144/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  71% 144/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  72% 145/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  72% 145/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  72% 146/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  72% 146/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  73% 147/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  73% 147/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  73% 148/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  73% 148/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  74% 149/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  74% 149/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  74% 150/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.output.dense.bias]      \u001b[A\n",
            "Loading weights:  74% 150/202 [00:00<00:00, 695.37it/s, Materializing param=deberta.encoder.layer.8.output.dense.bias]\u001b[A\n",
            "Loading weights:  75% 151/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.8.output.dense.bias]\u001b[A\n",
            "Loading weights:  75% 151/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.8.output.dense.weight]\u001b[A\n",
            "Loading weights:  75% 151/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.8.output.dense.weight]\u001b[A\n",
            "Loading weights:  75% 152/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  75% 152/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  76% 153/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  76% 153/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  76% 154/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  76% 154/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  77% 155/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  77% 155/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  77% 156/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  77% 156/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  78% 157/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  78% 157/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  78% 158/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  78% 158/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  79% 159/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  79% 159/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  79% 160/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  79% 160/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  80% 161/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  80% 161/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  80% 162/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  80% 162/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  81% 163/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  81% 163/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  81% 164/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  81% 164/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  82% 165/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  82% 165/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  82% 166/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.dense.bias]      \u001b[A\n",
            "Loading weights:  82% 166/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.dense.bias]\u001b[A\n",
            "Loading weights:  83% 167/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.dense.weight]\u001b[A\n",
            "Loading weights:  83% 167/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.9.output.dense.weight]\u001b[A\n",
            "Loading weights:  83% 168/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  83% 168/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  84% 169/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  84% 169/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  84% 170/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  84% 170/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  85% 171/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  85% 171/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  85% 172/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  85% 172/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  86% 173/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  86% 173/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  86% 174/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  86% 174/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  87% 175/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  87% 175/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  87% 176/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  87% 176/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  88% 177/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  88% 177/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  88% 178/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  88% 178/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  89% 179/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  89% 179/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  89% 180/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  89% 180/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  90% 181/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  90% 181/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  90% 182/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.dense.bias]      \u001b[A\n",
            "Loading weights:  90% 182/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.dense.bias]\u001b[A\n",
            "Loading weights:  91% 183/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.dense.weight]\u001b[A\n",
            "Loading weights:  91% 183/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.10.output.dense.weight]\u001b[A\n",
            "Loading weights:  91% 184/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  91% 184/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  92% 185/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  92% 185/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  92% 186/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.dense.bias]      \u001b[A\n",
            "Loading weights:  92% 186/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.dense.bias]\u001b[A\n",
            "Loading weights:  93% 187/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  93% 187/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.output.dense.weight]\u001b[A\n",
            "Loading weights:  93% 188/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.key_proj.bias] \u001b[A\n",
            "Loading weights:  93% 188/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.key_proj.bias]\u001b[A\n",
            "Loading weights:  94% 189/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  94% 189/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.key_proj.weight]\u001b[A\n",
            "Loading weights:  94% 190/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  94% 190/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.query_proj.bias]\u001b[A\n",
            "Loading weights:  95% 191/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  95% 191/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.query_proj.weight]\u001b[A\n",
            "Loading weights:  95% 192/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.value_proj.bias]  \u001b[A\n",
            "Loading weights:  95% 192/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.value_proj.bias]\u001b[A\n",
            "Loading weights:  96% 193/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  96% 193/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.attention.self.value_proj.weight]\u001b[A\n",
            "Loading weights:  96% 194/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.intermediate.dense.bias]         \u001b[A\n",
            "Loading weights:  96% 194/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.intermediate.dense.bias]\u001b[A\n",
            "Loading weights:  97% 195/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  97% 195/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.intermediate.dense.weight]\u001b[A\n",
            "Loading weights:  97% 196/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.LayerNorm.bias]    \u001b[A\n",
            "Loading weights:  97% 196/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.LayerNorm.bias]\u001b[A\n",
            "Loading weights:  98% 197/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  98% 197/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.LayerNorm.weight]\u001b[A\n",
            "Loading weights:  98% 198/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.dense.bias]      \u001b[A\n",
            "Loading weights:  98% 198/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.dense.bias]\u001b[A\n",
            "Loading weights:  99% 199/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.dense.weight]\u001b[A\n",
            "Loading weights:  99% 199/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.layer.11.output.dense.weight]\u001b[A\n",
            "Loading weights:  99% 200/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.rel_embeddings.weight]       \u001b[A\n",
            "Loading weights:  99% 200/202 [00:00<00:00, 632.16it/s, Materializing param=deberta.encoder.rel_embeddings.weight]\u001b[A\n",
            "Loading weights: 100% 201/202 [00:00<00:00, 632.16it/s, Materializing param=pooler.dense.bias]                    \u001b[A\n",
            "Loading weights: 100% 201/202 [00:00<00:00, 632.16it/s, Materializing param=pooler.dense.bias]\u001b[A\n",
            "Loading weights: 100% 202/202 [00:00<00:00, 632.16it/s, Materializing param=pooler.dense.weight]\u001b[A\n",
            "Loading weights: 100% 202/202 [00:00<00:00, 686.54it/s, Materializing param=pooler.dense.weight]\n",
            "model.safetensors:  66% 655M/990M [00:11<00:03, 88.4MB/s]\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: yangheng/deberta-v3-base-absa-v1.1\n",
            "Key                             | Status     |  | \n",
            "--------------------------------+------------+--+-\n",
            "deberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "2026-02-06 18:19:08,244 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:08,268 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:08,292 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "\n",
            "tokenizer_config.json: 100% 372/372 [00:00<00:00, 464kB/s]\n",
            "model.safetensors:  73% 722M/990M [00:12<00:02, 114MB/s] 2026-02-06 18:19:08,347 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:08,403 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "model.safetensors:  80% 789M/990M [00:12<00:01, 148MB/s]2026-02-06 18:19:08,465 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/spm.model \"HTTP/1.1 302 Found\"\n",
            "\n",
            "model.safetensors:  93% 923M/990M [00:12<00:00, 202MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 3.74MB/s]\n",
            "model.safetensors: 100% 990M/990M [00:13<00:00, 76.0MB/s]\n",
            "2026-02-06 18:19:09,193 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/tokenizer.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:09,357 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/added_tokens.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:09,373 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/added_tokens.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:09,391 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/added_tokens.json \"HTTP/1.1 200 OK\"\n",
            "added_tokens.json: 100% 18.0/18.0 [00:00<00:00, 80.5kB/s]\n",
            "2026-02-06 18:19:09,438 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:09,454 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:09,472 - INFO - HTTP Request: GET https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
            "special_tokens_map.json: 100% 156/156 [00:00<00:00, 535kB/s]\n",
            "2026-02-06 18:19:09,527 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:10,088 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,089 - INFO - Initializing Instruct-DeBERTa (Beam Size = 3) [Hyperparameter Study]...\n",
            "[Instruct-DeBERTa] Loading on cpu (Beam Size: 3)...\n",
            "2026-02-06 18:19:10,142 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:10,158 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,217 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:10,234 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,281 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:10,330 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,609 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:10,626 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,678 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:10,695 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,761 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/model.safetensors \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:10,810 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:10,878 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/commits/main \"HTTP/1.1 200 OK\"\n",
            "Loading weights:  28% 79/284 [00:00<00:00, 966.73it/s, Materializing param=decoder.block.5.layer.1.EncDecAttention.q.weight]2026-02-06 18:19:10,946 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/discussions?p=0 \"HTTP/1.1 200 OK\"\n",
            "Loading weights:  54% 154/284 [00:00<00:00, 1346.72it/s, Materializing param=decoder.block.10.layer.2.DenseReluDense.wo.weight]2026-02-06 18:19:11,049 - INFO - HTTP Request: GET https://huggingface.co/api/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/commits/refs%2Fpr%2F1 \"HTTP/1.1 200 OK\"\n",
            "Loading weights:  80% 228/284 [00:00<00:00, 1346.72it/s, Materializing param=encoder.block.6.layer.0.SelfAttention.o.weight]2026-02-06 18:19:11,137 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/refs%2Fpr%2F1/model.safetensors.index.json \"HTTP/1.1 404 Not Found\"\n",
            "Loading weights: 100% 284/284 [00:00<00:00, 930.41it/s, Materializing param=shared.weight]\n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie shared.weight to decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "2026-02-06 18:19:11,241 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/refs%2Fpr%2F1/model.safetensors \"HTTP/1.1 302 Found\"\n",
            "2026-02-06 18:19:11,311 - INFO - HTTP Request: HEAD https://huggingface.co/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/resolve/main/generation_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:11,329 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/kevinscaria/ate_tk-instruct-base-def-pos-neg-neut-combined/c1ba0cf3f059c8cd7db247b71f3b8ce654ef68d2/generation_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:11,385 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:11,402 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:11,455 - INFO - HTTP Request: HEAD https://huggingface.co/yangheng/deberta-v3-base-absa-v1.1/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:11,474 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/yangheng/deberta-v3-base-absa-v1.1/10c9dff335a44073e1352360c3a7bc54dc58eb01/config.json \"HTTP/1.1 200 OK\"\n",
            "Loading weights: 100% 202/202 [00:00<00:00, 660.85it/s, Materializing param=pooler.dense.weight]\n",
            "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: yangheng/deberta-v3-base-absa-v1.1\n",
            "Key                             | Status     |  | \n",
            "--------------------------------+------------+--+-\n",
            "deberta.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "2026-02-06 18:19:11,969 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:12,023 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:12,682 - INFO - HTTP Request: GET https://huggingface.co/api/models/yangheng/deberta-v3-base-absa-v1.1 \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:12,684 - INFO - Initializing Baseline (DistilBERT)...\n",
            "[Baseline] Loading DistilBERT on cpu...\n",
            "2026-02-06 18:19:12,735 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:12,781 - INFO - HTTP Request: GET https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "config.json: 100% 629/629 [00:00<00:00, 2.13MB/s]\n",
            "2026-02-06 18:19:12,836 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:12,919 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:12,974 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
            "2026-02-06 18:19:13,023 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english/xet-read-token/714eb0fa89d2f80546fda750413ed43d93601a13 \"HTTP/1.1 200 OK\"\n",
            "model.safetensors: 100% 268M/268M [00:01<00:00, 171MB/s]\n",
            "Loading weights: 100% 104/104 [00:00<00:00, 977.70it/s, Materializing param=pre_classifier.weight]\n",
            "2026-02-06 18:19:14,803 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:14,854 - INFO - HTTP Request: GET https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 194kB/s]\n",
            "2026-02-06 18:19:14,904 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert-base-uncased-finetuned-sst-2-english/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:14,947 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:14,993 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert-base-uncased-finetuned-sst-2-english/tree/main?recursive=true&expand=false \"HTTP/1.1 307 Temporary Redirect\"\n",
            "2026-02-06 18:19:15,043 - INFO - HTTP Request: GET https://huggingface.co/api/models/distilbert/distilbert-base-uncased-finetuned-sst-2-english/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:15,088 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/vocab.txt \"HTTP/1.1 200 OK\"\n",
            "2026-02-06 18:19:15,148 - INFO - HTTP Request: GET https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/vocab.txt \"HTTP/1.1 200 OK\"\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 6.24MB/s]\n",
            "2026-02-06 18:19:15,237 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/tokenizer.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:15,286 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:15,339 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/special_tokens_map.json \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:15,389 - INFO - HTTP Request: HEAD https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
            "2026-02-06 18:19:15,414 - INFO - Evaluating Proposed Model (Beam=1)...\n",
            "2026-02-06 18:19:30,614 - INFO - Results V1: {'ATE_F1': 0.2581, 'ASC_Accuracy': 0.75, 'ASC_F1': 0.7667}\n",
            "2026-02-06 18:19:30,614 - INFO - Evaluating Proposed Model (Beam=3)...\n",
            "2026-02-06 18:19:48,543 - INFO - Results V2: {'ATE_F1': 0.0741, 'ASC_Accuracy': 1.0, 'ASC_F1': 1.0}\n",
            "2026-02-06 18:19:48,544 - INFO - Evaluating Baseline...\n",
            "2026-02-06 18:19:49,094 - INFO - Results Baseline: {'ATE_F1': 0.0, 'ASC_Accuracy': 0.8125, 'ASC_F1': 0.8118}\n",
            "2026-02-06 18:19:49,103 - INFO - Summary metrics saved to results/summary_metrics_20260206_181949.csv\n",
            "2026-02-06 18:19:49,103 - INFO - Detailed predictions saved to results/detailed_predictions_20260206_181949.csv\n",
            "\n",
            "--- FINAL RESULTS TABLE ---\n",
            "                    Model  ATE_F1  ASC_Accuracy  ASC_F1\n",
            "Baseline (Sentence-Level)     N/A        0.8125  0.8118\n",
            "Instruct-DeBERTa (Beam=1)  0.2581        0.7500  0.7667\n",
            "Instruct-DeBERTa (Beam=3)  0.0741        1.0000  1.0000\n"
          ]
        }
      ],
      "source": [
        "! python3 main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sDVTkeaC29u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
